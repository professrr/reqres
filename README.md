# reqres
Reqres.in scrapper
* Frontend - Vue3.js
* API - Node.js & Express
* Scrapper - Node.js
* MQ - RabbitMQ
* DB - MongoDB

## Как это работает?

* Пользователь нажимает кнопку «запустить кроулер» и дергает наш API
* Наш API дергает воркер кроулера, используя RabbitMQ. 
* Воркер кроулера ловит задачу.
* Кроулер начинает работу, делает 2 асинхронных одновременных запроса (каждый запрос об одном пользователе)
* Каждый из пользователей сохраняется в MongoDB, при учете что его id не встречался раньше
* Кроулер ждет 1 минуту
(Если до этого момента мы нажали кнопку «Остановить кроулер» - то он перейдет в режим ожидания, в ином случае через 1 минуту процесс кроулинга повторится заново) 
* Как только кроулер добудет инфу со всех страниц, то внутренний пагинатор обнулится и в таком случае кроулер будет повторять весь этот цикл 

```Почему 2 асинхронных запроса? На самом деле можно и одним запросом всех пользователей забрать, но какой тогда смысл повторять итерацию через минуту. Поэтому 2 асинхронных запроса это просто мой выбор чтобы показать работоспособность асинхронного кроулинга данных. Конфиг кроулера можно поменять как угодно, в этом плане он гибкий```

### Что будет если?
- Нажать на кнопку "запустить/остановить кроулер" несколько раз. Ничего плохого не будет, кроулер проверяет состояние только после сохранения пользователейИ и если последним нажатием была кнопка "остановить" то кроулер перейдет в режим ожидания

## Запуск
Для запуска используйте следующие команды, при этом config.env - единая точка для всех переменных окружения:

```bash
sudo docker-compose --env-file ./config.env build

sudo docker-compose --env-file ./config.env up -d
```

Теперь чтобы перейти в интерфейс фронтенда используйте http://localhost:4444

## Остальное
При остановке запущеного процесса кроулинга - кроулинг продолжится пока не сохранит документы

```bash
# Чтобы остановить контейнера
sudo docker-compose --env-file ./config.env stop

# Логи 
sudo docker logs -f -n 1000 <CONTAINER ID>

```